%The normalization and standardization, in case of the missing of the raw
%data, I choose to ding, dealing with missing data, feature stan-dardization 
load('D_Train.mat') 
load('D_Test.mat') 
data_train=Train{:,1:8};
data_test=Test{:,1:8};
feature_train=Train{:,2:8};
feature_test=Test{:,2:8};
L=length(feature_train);
means=sum(feature_train)/L;
std=sqrt(sum((feature_train-means).^2))/L;
nor_train=(feature_train-means)./std;
nor_test=(feature_test-means)./std;
%seperate label and feature
label_train=Train{:,1};
label_test=Test{:,1};
%creat dataset
Train_dataset=prdataset(nor_train,label_train);   
Test_dataset=prdataset(nor_test,label_test);
Trainx=prdataset(feature_train,label_train);
Testx=prdataset(feature_test,label_test);
data=[label_train nor_train];
dataTest=[label_test nor_test];
%FLD DATASET The Minimum least square classifier with modified feature dimension
MLSmapping=fisherm(Train_dataset,3);
TestFLD = prmap(Test_dataset,MLSmapping);
TrainFLD = prmap(Train_dataset,MLSmapping);
MLSmapping=fisherm(Trainx,3);
TestFLDx = prmap(Testx,MLSmapping);
TrainFLDx = prmap(Trainx,MLSmapping);
%Firstly, prdataset is a function in prtools5, which could create a dataset
%each row is a data point with feature value, each column is a feature.
%I want to get the classifier simply use the given function in prtools5
%The Minimum least square linear classifier
Classifier_knn=knnc(Train_dataset);
Classifier_knnx=knnc(Trainx);
%Check the error rate 
E_knn=testc(Test_dataset*Classifier_knn);
E_knnx=testc(Testx*Classifier_knnx);
%For mls fld
Classifier_knnFLD=knnc(TrainFLD);
E_knnFLD=testc(TestFLD*Classifier_knnFLD);
Classifier_knnFLDx=knnc(TrainFLDx);
E_knnFLDx=testc(TestFLDx*Classifier_knnFLDx);
Etolx=0;
Eminx=1;
indices=crossvalind('Kfold',data_train(1:1600,8),10);
for k=1:10
    test=(indices==k);
    train=~test;
    train_data=data(train,:);
    test_data=data(test,:);
    ftrain=train_data(:,2:8);
    ftest=test_data(:,2:8);
    ltrain=train_data(:,1);
    ltest=test_data(:,1);
    Train_dataset1x=prdataset(ftrain,ltrain);
    Test_dataset1x=prdataset(ftest, ltest);
    Classifier_knn1x=knnc(Train_dataset1x,k);
    E_knnx=testc(Test_dataset1x*Classifier_knn1x);
    Etolx=Etolx+E_knnx;
    if E_knnx<Eminx
        Eminx= E_knnx;
        kminx=k;
    end
end
Eavgx=Etolx/10;
Etol=0;
Emin=1;
indices=crossvalind('Kfold',data(1:1600,8),10);
for k=1:10
    test=(indices==k);
    train=~test;
    train_data=data(train,:);
    test_data=data(test,:);
    ftrain=train_data(:,2:8);
    ftest=test_data(:,2:8);
    ltrain=train_data(:,1);
    ltest=test_data(:,1);
    Train_dataset1=prdataset(ftrain,ltrain);
    Test_dataset1=prdataset(ftest, ltest);
    Classifier_knn1=knnc(Train_dataset1,k);
    E_knn=testc(Test_dataset1*Classifier_knn1);
    Etol=Etol+E_knn;
    if E_knn<Emin
        Emin= E_knn;
        kmin=k;
    end
    if k==7
        predicted=labeld(ftest,Classifier_knn1);
        confusion_matrix=confusionmat(ltest,predicted);
    end
end
 
Eavg=Etol/10;
indices=crossvalind('Kfold',data(1:1600,8),10);
Etol1=0;
Emin1=1;
for k=1:10
    test=(indices==k);
    train=~test;
    train_data=data(train,:);
    test_data=data(test,:);
    ftrain=train_data(:,2:8);
    ftest=test_data(:,2:8);
    ltrain=train_data(:,1);
    ltest=test_data(:,1);
    Train_dataset1=prdataset(ftrain,ltrain);
    Test_dataset1=prdataset(ftest, ltest);
    MLSmapping1=fisherm(Train_dataset1,3);
    TestFLD1 = prmap(Test_dataset1,MLSmapping1);
    TrainFLD1 = prmap(Train_dataset1,MLSmapping1);
    Classifier_knn2=knnc(TrainFLD1,k);
    E_knn1=testc(TestFLD1*Classifier_knn2);
    Etol1=Etol1+E_knn1;
    if E_knn1<Emin1
        Emin1= E_knn1;
        kmin1=k;
    end
end
Eavg1=Etol1/10;
E_knn    
E_knnx
E_knnFLD
E_knnFLDx
Eavgx
Eavg
Eavg1
Eminx
Emin
Emin1
